{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version Control Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"V2\"\n",
    "addInfo = \"Model1_mitSmote\"\n",
    "date = '22-05-2024'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changelog\n",
    "\n",
    "###### added window 3 to the list #####\n",
    "###### added two additional Indicators (ATR und BollingerBands) ######\n",
    "# added SMOTE Oversampling\n",
    "# trained CNN 1 with 100 Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from ta.trend import TRIXIndicator\n",
    "from ta.trend import WMAIndicator\n",
    "from ta.trend import CCIIndicator\n",
    "from ta.volume import money_flow_index\n",
    "import pandas_ta as pta\n",
    "from ta.trend import MACD\n",
    "from ta.momentum import PercentagePriceOscillator\n",
    "from ta.momentum import ROCIndicator\n",
    "from ta.volume import ChaikinMoneyFlowIndicator\n",
    "from ta.trend import ADXIndicator\n",
    "from ta.volatility import AverageTrueRange\n",
    "from ta.volatility import BollingerBands\n",
    "\n",
    "import joblib\n",
    "\n",
    "# a simple CNN model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of stocks which should be in the training data\n",
    "\n",
    "#stocks = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'NVDA', 'PYPL', 'INTC', 'ADBE']\n",
    "\n",
    "stocks = ['MMM', 'ABT', 'ABBV', 'ACN', 'AYI', 'ADBE', 'AMD', 'AAP', 'AES', 'AET', 'AMG', 'AFL', 'A', 'APD', 'AKAM', 'ALK', 'ALB', 'ARE', 'ALGN', 'ALLE', 'LNT', 'ALL', 'GOOGL', 'GOOG', 'MO', 'AMZN', 'AEE', 'AAL', 'AEP', 'AXP', 'AIG', 'AMT', 'AWK', 'AMP', 'AME', 'AMGN', 'APH', 'ADI', 'ANDV', 'ANSS', 'AON', 'AOS', 'APA', 'AIV', 'AAPL', 'AMAT', 'ADM', 'AJG', 'AIZ', 'T', 'ADSK', 'ADP', 'AZO', 'AVB', 'AVY', 'BAC', 'BK', 'BAX', 'BDX', 'BBY', 'BIIB', 'BLK', 'HRB', 'BA', 'BWA', 'BXP', 'BSX', 'BHF', 'BMY', 'AVGO', 'CHRW', 'CPB', 'COF', 'CAH', 'CBOE', 'KMX', 'CCL', 'CAT', 'CNC', 'CNP', 'CF', 'SCHW', 'CHTR', 'CHK', 'CVX', 'CMG', 'CB', 'CHD', 'CI', 'CINF', 'CTAS', 'CSCO', 'C', 'CFG', 'CLX', 'CME', 'CMS', 'KO', 'CTSH', 'CL', 'CMCSA', 'CMA', 'CAG', 'COP', 'ED', 'STZ', 'COO', 'GLW', 'COST', 'COTY', 'CCI', 'CSRA', 'CSX', 'CMI', 'CVS', 'DHI', 'DHR', 'DRI', 'DVA', 'DE', 'DAL', 'XRAY', 'DVN', 'DLR', 'DFS', 'DG', 'DLTR', 'D', 'DOV', 'DOW', 'DTE', 'DD', 'DUK', 'DXC', 'EMN', 'ETN', 'EBAY', 'ECL', 'EIX', 'EW', 'EA', 'EMR', 'ETR', 'EVHC', 'EOG', 'EQT', 'EFX', 'EQIX', 'EQR', 'ESS', 'EL', 'ES', 'EXC', 'EXPE', 'EXPD', 'ESRX', 'EXR', 'XOM', 'FFIV', 'FAST', 'FRT', 'FDX', 'FIS', 'FITB', 'FE', 'FLS', 'FLR', 'FMC', 'FL', 'F', 'FTV', 'BEN', 'FCX', 'GPS', 'GRMN', 'IT', 'GD', 'GE', 'GIS', 'GM', 'GPC', 'GILD', 'GPN', 'GS', 'GT', 'GWW', 'HAL', 'HBI', 'HOG', 'HIG', 'HAS', 'HCA', 'HCP', 'HP', 'HSIC', 'HSY', 'HES', 'HPE', 'HLT', 'HOLX', 'HD', 'HON', 'HRL', 'HST', 'HPQ', 'HUM', 'HBAN', 'IDXX', 'ITW', 'ILMN', 'IR', 'INTC', 'ICE', 'IBM', 'INCY', 'IP', 'IPG', 'IFF', 'INTU', 'ISRG', 'IVZ', 'IRM', 'JBHT', 'SJM', 'JNJ', 'JCI', 'JPM', 'JNPR', 'K', 'KEY', 'KMB', 'KIM', 'KMI', 'KLAC', 'KSS', 'KHC', 'KR', 'LH', 'LRCX', 'LEG', 'LEN', 'LLY', 'LNC', 'LKQ', 'LMT', 'L', 'LOW', 'LYB', 'MTB', 'MAC', 'M', 'MRO', 'MPC', 'MAR', 'MMC', 'MLM', 'MAS', 'MA', 'MAT', 'MKC', 'MCD', 'MCK', 'MDT', 'MRK', 'MET', 'MTD', 'MGM', 'MCHP', 'MU', 'MSFT', 'MAA', 'MHK', 'TAP', 'MDLZ', 'MNST', 'MCO', 'MS', 'MOS', 'MSI', 'NDAQ', 'NOV', 'NAVI', 'NTAP', 'NFLX', 'NWL', 'NFX', 'NEM', 'NWSA', 'NWS', 'NEE', 'NKE', 'NI', 'JWN', 'NSC', 'NTRS', 'NOC', 'NRG', 'NUE', 'NVDA', 'ORLY', 'OXY', 'OMC', 'OKE', 'ORCL', 'PCAR', 'PKG', 'PH', 'PDCO', 'PAYX', 'PYPL', 'PNR', 'PEP', 'PRGO', 'PFE', 'PCG', 'PM', 'PSX', 'PNW', 'PXD', 'PNC', 'RL', 'PPG', 'PPL', 'PX', 'PFG', 'PG', 'PGR', 'PLD', 'PRU', 'PEG', 'PSA', 'PHM', 'PVH', 'QRVO', 'PWR', 'QCOM', 'DGX', 'RRC', 'RJF', 'O', 'REG', 'REGN', 'RF', 'RSG', 'RMD', 'RHI', 'ROK', 'COL', 'ROP', 'ROST', 'RCL', 'CRM', 'SCG', 'SLB', 'SNI', 'STX', 'SEE', 'SRE', 'SHW', 'SIG', 'SPG', 'SWKS', 'SLG', 'SNA', 'SO', 'LUV', 'SPGI', 'SWK', 'SBUX', 'STT', 'SRCL', 'SYK', 'STI', 'SYF', 'SNPS', 'SYY', 'TROW', 'TGT', 'TEL', 'FTI', 'TXN', 'TXT', 'TMO', 'TWX', 'TJX', 'TSCO', 'TDG', 'TRV', 'TRIP', 'FOXA', 'FOX', 'TSN', 'UDR', 'ULTA', 'USB', 'UA', 'UAA', 'UNP', 'UAL', 'UNH', 'UPS', 'URI', 'UHS', 'UNM', 'VFC', 'VLO', 'VTR', 'VRSN', 'VRSK', 'VZ', 'VRTX', 'V', 'VNO', 'VMC', 'WMT', 'WBA', 'DIS', 'WM', 'WAT', 'WEC', 'WFC', 'WDC', 'WU', 'WRK', 'WY', 'WHR', 'WMB', 'WYNN', 'XEL', 'XRX', 'XYL', 'YUM', 'ZBH', 'ZION', 'ZTS']\n",
    "print(len(stocks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data from Yahoo Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stock_data(symbols):\n",
    "    \"\"\"\n",
    "    Herunterladen von Aktiendaten mit yfinance und Speichern in CSV-Dateien.\n",
    "\n",
    "    Parameter:\n",
    "    - symbols: Eine Liste von Aktiensymbolen (z.B. ['AAPL', 'MSFT', 'GOOGL'])\n",
    "\n",
    "    Rückgabewert:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    for symbol in symbols:\n",
    "        data = yf.download(symbol, start= '2018-01-01', end='2022-12-31')\n",
    "        file_path = os.path.join('data', f'{symbol}.csv')\n",
    "        data.to_csv(file_path)\n",
    "        \n",
    "        print(f'Aktiendaten für {symbol} heruntergeladen und in {file_path} gespeichert.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_stock_data(stocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframes for each stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframes():\n",
    "    \"\"\"\n",
    "    Erstellt pro CSV-Datei einen DataFrame mit dem Namen der Aktie.\n",
    "\n",
    "    Rückgabewert:\n",
    "    - dataframes: Ein Dictionary, das die DataFrames enthält, wobei die Schlüssel die Aktiennamen sind.\n",
    "    \"\"\"\n",
    "    dataframes = {}\n",
    "    data_folder = 'data'\n",
    "\n",
    "    # Durchlaufen aller CSV-Dateien im `data`-Ordner\n",
    "    for file_name in os.listdir(data_folder):\n",
    "        if file_name.endswith('.csv'):\n",
    "            # Aktienname aus dem Dateinamen extrahieren (ohne '.csv')\n",
    "            stock_name = os.path.splitext(file_name)[0]\n",
    "            \n",
    "            # DataFrame aus der CSV-Datei erstellen und im Dictionary speichern\n",
    "            df = pd.read_csv(os.path.join(data_folder, file_name))\n",
    "            dataframes[stock_name] = df\n",
    "    return dataframes\n",
    "\n",
    "# Beispielaufruf\n",
    "dataframes = create_dataframes()\n",
    "print(dataframes.keys())  # Ausgabe der Aktiennamen, die als Schlüssel im Dictionary gespeichert sind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change Date to datetime\n",
    "for df in dataframes:\n",
    "    dataframes[df]['Date'] = pd.to_datetime(dataframes[df]['Date'])\n",
    "\n",
    "\n",
    "# set Date as index\n",
    "for df in dataframes:\n",
    "    dataframes[df].set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just to check\n",
    "\n",
    "print(len(stocks))\n",
    "print(len(dataframes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Template for feature creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NAME_FEATUERE(df):\n",
    "    #implementation of the feature-calculation\n",
    "    # directly add the feature to the dataframe\n",
    "    # no reurn needed\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = [3,5,7,9,11,13,15,17,19,21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Moving Average SMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SMA(df):\n",
    "    for i in window:\n",
    "        df[f\"SMA_{i}\"] = df['Close'].rolling(window=i).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function for the exponential moving average\n",
    "def get_EMA(df):\n",
    "    for i in window:\n",
    "        df[f\"EMA_{i}\"] = df['Close'].ewm(span=i, adjust=False).mean()   # adjust checken: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ewm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triple Exponential Moving Average (TEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TRIX(df):\n",
    "    for i in window:\n",
    "        trix_values = TRIXIndicator(df['Close'], i).trix()\n",
    "        df[f\"TRIX_{i}\"] = trix_values\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mony Flow Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Money Flow Index (MFI) is a technical oscillator that uses price and volume data for identifying overbought or oversold signals in an asset. It can also be used to spot divergences which warn of a trend change in price. The oscillator moves between 0 and 100.\n",
    "\n",
    "https://www.investopedia.com/terms/m/mfi.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Money Flow Index\n",
    "# Uses monex_flow_index from ta library\n",
    "\n",
    "def get_MFI(df):\n",
    "    for i in window:\n",
    "        df[f\"money_flow_index_{i}\"] = money_flow_index(df['High'], df['Low'], df['Close'], df['Volume'], window=i)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothed RSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RSI_smooth(df):\n",
    "    \"\"\"\n",
    "    Momentum indicator\n",
    "    As per https://www.investopedia.com/terms/r/rsi.asp\n",
    "    RSI_1 = 100 - (100/ (1 + (avg gain% / avg loss%) ) )\n",
    "    RSI_2 = 100 - (100/ (1 + (prev_avg_gain*13+avg gain% / prev_avg_loss*13 + avg loss%) ) )\n",
    "\n",
    "    E.g. if period==6, first RSI starts from 7th index because difference of first row is NA\n",
    "    http://cns.bu.edu/~gsc/CN710/fincast/Technical%20_indicators/Relative%20Strength%20Index%20(RSI).htm\n",
    "    https://school.stockcharts.com/doku.php?id=technical_indicators:relative_strength_index_rsi\n",
    "    Verified!\n",
    "    \"\"\"\n",
    "\n",
    "    prev_rsi = np.inf\n",
    "    prev_avg_gain = np.inf\n",
    "    prev_avg_loss = np.inf\n",
    "    rolling_count = 0\n",
    "\n",
    "    def calculate_RSI(series, period):\n",
    "        # nonlocal rolling_count\n",
    "        nonlocal prev_avg_gain\n",
    "        nonlocal prev_avg_loss\n",
    "        nonlocal rolling_count\n",
    "\n",
    "        # num_gains = (series >= 0).sum()\n",
    "        # num_losses = (series < 0).sum()\n",
    "        # sum_gains = series[series >= 0].sum()\n",
    "        # sum_losses = np.abs(series[series < 0].sum())\n",
    "        curr_gains = series.where(series >= 0, 0)  # replace 0 where series not > 0\n",
    "        curr_losses = np.abs(series.where(series < 0, 0))\n",
    "        avg_gain = curr_gains.sum() / period  # * 100\n",
    "        avg_loss = curr_losses.sum() / period  # * 100\n",
    "        rsi = -1\n",
    "\n",
    "        if rolling_count == 0:\n",
    "            # first RSI calculation\n",
    "            rsi = 100 - (100 / (1 + (avg_gain / avg_loss)))\n",
    "            # print(rolling_count,\"rs1=\",rs, rsi)\n",
    "        else:\n",
    "            # smoothed RSI\n",
    "            # current gain and loss should be used, not avg_gain & avg_loss\n",
    "            rsi = 100 - (100 / (1 + ((prev_avg_gain * (period - 1) + curr_gains.iloc[-1]) /\n",
    "                                     (prev_avg_loss * (period - 1) + curr_losses.iloc[-1]))))\n",
    "            # print(rolling_count,\"rs2=\",rs, rsi)\n",
    "\n",
    "        # df['rsi_'+str(period)+'_own'][period + rolling_count] = rsi\n",
    "        rolling_count = rolling_count + 1\n",
    "        prev_avg_gain = avg_gain\n",
    "        prev_avg_loss = avg_loss\n",
    "        return rsi\n",
    "\n",
    "    diff = df['Close'].diff()[1:]  # skip na\n",
    "    for i in window:\n",
    "        df['rsi_' + str(i)] = np.nan\n",
    "        # df['rsi_'+str(period)+'_own_1'] = np.nan\n",
    "        rolling_count = 0\n",
    "        res = diff.rolling(i).apply(calculate_RSI, args=(i,), raw=False)\n",
    "        df['rsi_' + str(i)][1:] = res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Williams %R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://de.wikipedia.org/wiki/Williams_%25R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_WILLIAMS_R(df):\n",
    "    for i in window:\n",
    "        highest_high = df['High'].rolling(window=i).max()\n",
    "        lowest_low = df['Low'].rolling(window=i).min()\n",
    "        df[f\"williams_r_{i}\"] = ((highest_high - df['Close']) / (highest_high - lowest_low)) * (-100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WMA - Weighted Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_WMA(df):\n",
    "    for i in window:\n",
    "        df[f\"wma_{i}\"] = WMAIndicator(df['Close'], i).wma()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMA - Hull Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ToDo: Formel und Resultate noch zu validieren!!\n",
    "\n",
    "def get_HMA(df):\n",
    "    \"\"\"\n",
    "    Berechnet den Hull Moving Average (HMA) und fügt ihn als neue Spalte zum DataFrame hinzu.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    for i in window:\n",
    "        wma1 = 2 * df['Close'].rolling(window=int(i / 2)).mean()\n",
    "        wma2 = df['Close'].rolling(window=i).mean()\n",
    "        diff = wma1 - wma2\n",
    "        hma = diff.rolling(window=int(np.sqrt(i))).mean()\n",
    "        df[f\"hma_{i}\"] = hma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCI - Commodity Channel Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CCI(df):\n",
    "    for i in window:\n",
    "        cci_values = CCIIndicator(df['High'], df['Low'], df['Close'], i).cci()\n",
    "        df[f\"cci_{i}\"] = cci_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMO - Chande Momentum Oszillator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/twopirllc/pandas-ta?tab=readme-ov-file#trend-18\n",
    "# ToDo: Check!\n",
    "\n",
    "def get_CMO(df):\n",
    "    for i in window:\n",
    "        df[f\"cmo_{i}\"] = pta.cmo(df['Close'], length=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MACD - Moving Average Convergence Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://www.lynxbroker.ch/boerse/trading/technische-analyse/technische-indikatoren/der-macd-indikator-bestimmen-sie-die-richtung-und-staerke-des-trends/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: Check and Implementation for different windows\n",
    "\n",
    "def get_MACD(df):\n",
    "    macd_object = MACD(df['Close'])\n",
    "    df['MACD'] = macd_object.macd()\n",
    "    df['MACD_Signal'] = macd_object.macd_signal()\n",
    "    df['MACD_Diff'] = macd_object.macd_diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO - Percentage Price Oscillator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://school.stockcharts.com/doku.php?id=technical_indicators:price_oscillators_ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: Check and Implementation for different windows\n",
    "\n",
    "def get_PPO(df):\n",
    "        ppo_object = PercentagePriceOscillator(df['Close'])\n",
    "        df[f\"PPO\"] = ppo_object.ppo()\n",
    "        df[f\"PPO_Histogram\"] = ppo_object.ppo_hist()\n",
    "        df[f\"PPO_Signal\"] = ppo_object.ppo_signal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC - Rate of Change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://school.stockcharts.com/doku.php?id=technical_indicators:rate_of_change_roc_and_momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ROC(df):\n",
    "    for i in window:\n",
    "        df[f\"ROC_{i}\"] = ROCIndicator(df['Close'], i).roc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMI - Chaikin Money Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://school.stockcharts.com/doku.php?id=technical_indicators:chaikin_money_flow_cmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CMF(df):\n",
    "    for i in window:\n",
    "        df[f\"cmf_{i}\"] = ChaikinMoneyFlowIndicator(df['High'], df['Low'], df['Close'], df['Volume'], i).chaikin_money_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADX - Average Directional Movement Index (ADX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:average_directional_index_adx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ADX(df):\n",
    "    for i in window:\n",
    "        df[f\"adx_{i}\"] = ADXIndicator(df['High'], df['Low'], df['Close'], i).adx()\n",
    "        df[f\"adx_pos_{i}\"] = ADXIndicator(df['High'], df['Low'], df['Close'], i).adx_pos()\n",
    "        df[f\"adx_neg_{i}\"] = ADXIndicator(df['High'], df['Low'], df['Close'], i).adx_neg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average True Range (ATR) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "für Volaitilität"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ATR(df):\n",
    "    for i in window:\n",
    "        df[f\"atr_{i}\"] = AverageTrueRange(df['High'], df['Low'], df['Close'], i).average_true_range()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bollinger Bands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "für Volatilität"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BollingerBands(df):\n",
    "    for i in window:\n",
    "        bb = BollingerBands(df['Close'], window=i, window_dev=2)\n",
    "        df[f\"bb_bbm_{i}\"] = bb.bollinger_mavg()\n",
    "        df[f\"bb_bbh_{i}\"] = bb.bollinger_hband()\n",
    "        df[f\"bb_bbl_{i}\"] = bb.bollinger_lband()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    get_SMA(df)\n",
    "    get_EMA(df)\n",
    "    get_TRIX(df)\n",
    "    get_MFI(df)\n",
    "    get_RSI_smooth(df)\n",
    "    get_WILLIAMS_R(df)\n",
    "    get_WMA(df)\n",
    "    get_HMA(df)\n",
    "    get_CCI(df)\n",
    "    get_CMO(df)\n",
    "    get_MACD(df)\n",
    "    get_PPO(df)\n",
    "    get_ROC(df)\n",
    "    get_CMF(df)\n",
    "    get_ADX(df)\n",
    "    get_ATR(df)\n",
    "    get_BollingerBands(df)\n",
    "    \n",
    "    #print('Feature creation done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate features for all dataframes\n",
    "\n",
    "for df in dataframes:\n",
    "    print(f'Calculating features for {df}...')\n",
    "    create_features(dataframes[df])\n",
    "    print(f'Features for {df} calculated.')\n",
    "\n",
    "print('All features calculated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['AAPL'].head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to lable the data\n",
    "# if i + 11 is the max value in the window, label as BUY\n",
    "# if i + 11 is the min value in the window, label as SELL\n",
    "# else label as HOLD\n",
    "# Idea: Buy when the stock is at its lowest in the window, sell when it is at its highest\n",
    "\n",
    "# Source: https://medium.com/@quantclubiitkgp/stock-buy-sell-hold-prediction-using-cnn-ee7b671f4ad3\n",
    "\n",
    "\n",
    "def get_labels_END(df, windowSize=11):\n",
    "   labels = [] \n",
    "   values = []\n",
    "   for i in range(len(df.Close) - windowSize): \n",
    "      mx = df.Close.iloc[i]\n",
    "      mn = df.Close.iloc[i]\n",
    "      mxIndex, mnIndex = i, i\n",
    "      for j in range(i + 1, i + windowSize + 1): \n",
    "         if df.Close.iloc[j] > mx:\n",
    "               mx = df.Close.iloc[j]\n",
    "               mxIndex = j\n",
    "         if df.Close.iloc[j] < mn: \n",
    "               mn = df.Close.iloc[j] \n",
    "               mnIndex = j\n",
    "\n",
    "      if mnIndex == i + 11:\n",
    "         labels.append('SELL')\n",
    "         values.append(i + 11)\n",
    "      elif mxIndex == i + 11:\n",
    "         labels.append('BUY')\n",
    "         values.append(i + 11)\n",
    "      else:\n",
    "         labels.append('HOLD')\n",
    "         values.append(i + 11)\n",
    "   \n",
    "   # Append 11 times a 999 to the end of the labels list\n",
    "   for i in range(11):\n",
    "      labels.append(999)\n",
    "\n",
    "   # Append the labels to the dataframe from the start\n",
    "   df['Labels_END'] = labels\n",
    "\n",
    "   # Replace the 999 with NaN\n",
    "   df['Labels_END'] = df['Labels_END'].replace(999, np.nan)\n",
    "\n",
    "   print('Labeling END done!')\n",
    "\n",
    "   return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://towardsdatascience.com/stock-market-action-prediction-with-convnet-8689238feae3\n",
    "\n",
    "\n",
    "def get_labels_MID(df, col_name, window_size=11):\n",
    "        \"\"\"\n",
    "        Data is labeled as per the logic in research paper\n",
    "        Label code : BUY => 1, SELL => 0, HOLD => 2\n",
    "        params :\n",
    "            df => Dataframe with data\n",
    "            col_name => name of column which should be used to determine strategy\n",
    "        returns : nothing - df is modified directly\n",
    "        \"\"\"\n",
    "\n",
    "        row_counter = 0\n",
    "        total_rows = len(df)\n",
    "        labels = np.zeros(total_rows)\n",
    "        labels[:] = np.nan\n",
    "\n",
    "        while row_counter < total_rows:\n",
    "            if row_counter >= window_size - 1:\n",
    "                window_begin = row_counter - (window_size - 1)\n",
    "                window_end = row_counter\n",
    "                window_middle = int((window_begin + window_end) / 2)\n",
    "\n",
    "                min_ = np.inf\n",
    "                min_index = -1\n",
    "                max_ = -np.inf\n",
    "                max_index = -1\n",
    "                for i in range(window_begin, window_end + 1):\n",
    "                    price = df.iloc[i][col_name]\n",
    "                    if price < min_:\n",
    "                        min_ = price\n",
    "                        min_index = i\n",
    "                    if price > max_:\n",
    "                        max_ = price\n",
    "                        max_index = i\n",
    "\n",
    "                if max_index == window_middle:\n",
    "                    labels[window_middle] = 0\n",
    "                elif min_index == window_middle:\n",
    "                    labels[window_middle] = 1\n",
    "                else:\n",
    "                    labels[window_middle] = 2\n",
    "\n",
    "            row_counter = row_counter + 1\n",
    "\n",
    "        # Add labels to the dataframe\n",
    "        df['Labels_MID'] = labels\n",
    "\n",
    "        # Replace the 0, 1, 2 with BUY, SELL, HOLD\n",
    "        df['Labels_MID'] = df['Labels_MID'].replace(0, 'SELL')\n",
    "        df['Labels_MID'] = df['Labels_MID'].replace(1, 'BUY')\n",
    "        df['Labels_MID'] = df['Labels_MID'].replace(2, 'HOLD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Labels for all dataframes\n",
    "i = 1\n",
    "for df in dataframes:\n",
    "    get_labels_MID(dataframes[df], 'Close')\n",
    "    print(f'Labels for {df} created. -- {i}/{len(dataframes)}')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_labels_per_stock(dataframes, stock_names):\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, stock_name in enumerate(stock_names):\n",
    "        df = dataframes.get(stock_name)  # DataFrame für die aktuelle Aktie erhalten\n",
    "        if df is None:\n",
    "            print(f\"Dataframe for {stock_name} not found.\")\n",
    "            continue\n",
    "\n",
    "        ax = axs[i]\n",
    "        df['Labels_MID'].value_counts().plot(kind='bar', ax=ax, title=stock_name)\n",
    "\n",
    "    fig.suptitle('Labels pro Aktie', fontsize=16, y=1.0005)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # save the plot\n",
    "    plt.savefig('Plots/labels_per_stock.png')\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Aufruf der Funktion\n",
    "stock_name_plot = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'PYPL', 'INTC', 'ADBE'] #Liste der Aktien, die geplottet werden sollen\n",
    "plot_labels_per_stock(dataframes, stock_name_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_stock_price(dataframes, stock_names):\n",
    "    num_plots = len(stock_names)\n",
    "    num_rows = (num_plots + 2) // 3 \n",
    "\n",
    "    fig, axs = plt.subplots(num_rows, 3, figsize=(18, 4 * num_rows))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, stock_name in enumerate(stock_names):\n",
    "        df = dataframes.get(stock_name)  # DataFrame für die aktuelle Aktie erhalten\n",
    "        if df is None:\n",
    "            print(f\"Dataframe for {stock_name} not found.\")\n",
    "            continue\n",
    "\n",
    "        ax = axs[i]\n",
    "        buy = df[df['Labels_MID'] == 'BUY']\n",
    "        sell = df[df['Labels_MID'] == 'SELL']\n",
    "        ax.scatter(buy.index, buy['Close'], color='g', label='Buy Signal', marker='^', alpha=1)\n",
    "        ax.scatter(sell.index, sell['Close'], color='r', label='Sell Signal', marker='v', alpha=1)\n",
    "\n",
    "        ax.plot(df['Close'], label='Close Price history')\n",
    "        ax.set_title(f'{stock_name}')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Price')\n",
    "        ax.legend(loc='upper left')\n",
    "        ax.grid()\n",
    "        fig.suptitle('BUY & SELL for each title', fontsize=18, y=1.0005)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Aufruf der Funktion\n",
    "stock_name_plot = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'PYPL', 'INTC', 'ADBE', 'ZION'] #Liste der Aktien, die geplottet werden sollen\n",
    "plot_stock_price(dataframes, stock_name_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a single plot\n",
    "\n",
    "def plot_single_stock_price(stock_name):\n",
    "    df = dataframes[stock_name]\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    buy = df[df['Labels_MID'] == 'BUY']\n",
    "    sell = df[df['Labels_MID'] == 'SELL']\n",
    "    ax.scatter(buy.index, buy['Close'], color='g', label='Buy Signal', marker='^', alpha=1)\n",
    "    ax.scatter(sell.index, sell['Close'], color='r', label='Sell Signal', marker='v', alpha=1)\n",
    "\n",
    "    ax.plot(df['Close'], label='Close Price history')\n",
    "    ax.set_title(f'BUY & SELL for {stock_name}')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Price')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid()\n",
    "\n",
    "    # save the plot\n",
    "    plot_name = f\"Plots/Labeling_{stock_name}.png\"\n",
    "    plt.savefig(plot_name)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Aufruf der Funktion\n",
    "plot_single_stock_price('AMZN')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason behind having this step in our pipeline is that Feature engineering involves creating, transforming, or selecting the most relevant variables in your dataset to improve model performance. This process is critical because it enables the model to learn from the data more effectively, leading to better predictions and insights. Feature selection, in particular, trims down the feature set to keep only the most valuable attributes, reducing complexity, computation time, and the risk of overfitting. In essence, these techniques streamline the modelling process, making it more accurate, interpretable, and efficient, while also saving time and resources.\n",
    "First, we Normalize our data set because Normalizing a dataset is crucial to ensure that all features are on a consistent scale, preventing one feature from dominating the analysis, improving model convergence, aiding in the interpretation of feature importance, making distance-based algorithms more reliable, and aligning with assumptions of regularisation techniques, all of which contribute to more effective and robust data analysis and machine learning.\n",
    "Then we will just apply a small filter of variance with a threshold of 0.1 i.e. any feature which has a variance less than or equal to 0.1 will be removed from the dataset as having this low variance does not contribute much to detecting the trend in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning individual dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the NaN values and 0.00 from the dataframe\n",
    "for i in dataframes:\n",
    "    dataframes[i].dropna(inplace=True)\n",
    "    dataframes[i] = dataframes[i][dataframes[i]['Close'] != 0.00]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the dataframes to one dataframe\n",
    "# save it as a csv file\n",
    "data_name = f\"merged_data/merged_data_large_{version}.csv\"\n",
    "merged_df = pd.concat(dataframes)\n",
    "merged_df.to_csv(data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the merged data\n",
    "merged_df = pd.read_csv('merged_data/merged_data_large_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels\n",
    "# Manual encoding\n",
    "\n",
    "merged_df['Labels_MID'] = merged_df['Labels_MID'].replace('BUY', 0)\n",
    "merged_df['Labels_MID'] = merged_df['Labels_MID'].replace('SELL', 1)\n",
    "merged_df['Labels_MID'] = merged_df['Labels_MID'].replace('HOLD', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of possible amount of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the number of features\n",
    "\n",
    "def calc_feature_number(features):\n",
    "    features = len(features)\n",
    "    return np.floor(np.sqrt(features)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_features = list(merged_df.loc[:, 'Open':'bb_bbl_21'].columns)\n",
    "print('Total number of features: ', len(list_features))\n",
    "print('Largest number of features that can be used in the model: ', calc_feature_number(list_features))\n",
    "print(f\"k = {calc_feature_number(list_features)**2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Valid - Test Split and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### just copied from: https://towardsdatascience.com/stock-market-action-prediction-with-convnet-8689238feae3\n",
    "### needs to be reworked and adapted to the features we want to use\n",
    "### create function for this\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "df = merged_df.dropna()\n",
    "\n",
    "list_features = list(df.loc[:, 'Open':'bb_bbl_21'].columns)  # needs to be changed to the features you want to use\n",
    "print('Total number of features', len(list_features))\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.loc[:, 'Open':'bb_bbl_21'], df['Labels_MID'], train_size=0.8, \n",
    "                                                    test_size=0.2, random_state=2, shuffle=True, stratify=df['Labels_MID'])\n",
    "\n",
    "train_split = 0.8\n",
    "\n",
    "print('train_split =',train_split)\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train, train_size=train_split, test_size=1-train_split, \n",
    "                                                random_state=2, shuffle=True, stratify=y_train)\n",
    "mm_scaler = MinMaxScaler(feature_range=(0, 1)) # or StandardScaler?\n",
    "x_train = mm_scaler.fit_transform(x_train)\n",
    "x_cv = mm_scaler.transform(x_cv)\n",
    "x_test = mm_scaler.transform(x_test)\n",
    "\n",
    "print(\"Shape of x, y train/cv/test {} {} {} {} {} {}\".format(x_train.shape, y_train.shape, x_cv.shape, y_cv.shape, x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the scaler\n",
    "scaler_name = f\"models/scaler_{version}_{addInfo}.pkl\"\n",
    "joblib.dump(mm_scaler, scaler_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create artificial data with SMOTE\n",
    "oversample = SMOTE()\n",
    "x_train, y_train = oversample.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of records in each class\n",
    "count = Counter(y_train)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the main part is that we will calculate the importance of each feature by using Random Forest Classifier and then reduce the feature size to 81.\n",
    "\n",
    "A rough idea of how random forest works is for each decision point (split) in each tree, the Random Forest algorithm measures the decrease in impurity(Gini impurity) resulting from the split. The impurity reduction brought about by each feature is averaged across all the trees in the forest. Features that consistently reduce impurity more effectively when used in splits are considered more important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Anwenden der Feature Selection mit SelectKBest und Chi-Quadrat-Test\n",
    "k = 100  # Anzahl der gewünschten Merkmale\n",
    "selector = SelectKBest(chi2, k=k)\n",
    "X_new = selector.fit_transform(x_train, y_train)\n",
    "\n",
    "# Index der ausgewählten Merkmale\n",
    "selected_feature_indices = selector.get_support(indices=True)\n",
    "\n",
    "# Ausgabe der Indizes der ausgewählten Merkmale\n",
    "print(\"Indizes der ausgewählten Merkmale:\")\n",
    "print(selected_feature_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save selected_feature_indices in a txt file\n",
    "np.savetxt(f\"models/selected_features_{version}_{addInfo}.txt\", selected_feature_indices, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(selected_feature_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use the selected features\n",
    "x_train = x_train[:, selected_feature_indices]\n",
    "x_cv = x_cv[:, selected_feature_indices]\n",
    "x_test = x_test[:, selected_feature_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reshape the data\n",
    "# Creates a 3D array (x, y and images for each row) for the CNN\n",
    "\n",
    "def reshape_as_image(x, img_width, img_height):\n",
    "    x_temp = np.zeros((len(x), img_height, img_width))\n",
    "    for i in range(x.shape[0]):\n",
    "        # print(type(x), type(x_temp), x.shape)\n",
    "        x_temp[i] = np.reshape(x[i], (img_height, img_width))\n",
    "\n",
    "    return x_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = int(np.sqrt(k))\n",
    "x_train = reshape_as_image(x_train, dim, dim)\n",
    "x_cv = reshape_as_image(x_cv, dim, dim)\n",
    "x_test = reshape_as_image(x_test, dim, dim)\n",
    "# adding a 1-dim for channels (3)\n",
    "x_train = np.stack((x_train,) * 3, axis=-1)\n",
    "x_test = np.stack((x_test,) * 3, axis=-1)\n",
    "x_cv = np.stack((x_cv,) * 3, axis=-1)\n",
    "\n",
    "print(f'Shape of x_train: {x_train.shape}')\n",
    "print(f'Shape of x_cv: {x_cv.shape}')\n",
    "print(f'Shape of x_test: {x_test.shape}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 6, figsize=(15, 10))\n",
    "\n",
    "for i in range(6):\n",
    "    sample = np.random.randint(0, x_train.shape[0])\n",
    "    axs[i].imshow(x_train[sample])\n",
    "    axs[i].set_title(y_train.iloc[sample])\n",
    "    axs[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.suptitle('Example Images with Classes', fontsize=18, y=0.68)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute class weights\n",
    "counter = Counter(y_train)\n",
    "max_val = float(max(counter.values()))\n",
    "class_weights = {class_id : max_val/num_images for class_id, num_images in counter.items()}\n",
    "print('Class weights: ', class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a array from y_train and y_cv\n",
    "y_train = np.array(y_train)\n",
    "y_cv = np.array(y_cv)\n",
    "y_test = np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple Baseline CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(16, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'],)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='auto', restore_best_weights = True, start_from_epoch = 50)\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    validation_data=(x_cv, y_cv),\n",
    "                    batch_size=64,\n",
    "                    epochs=300,\n",
    "                    callbacks=[early_stopping],\n",
    "                    class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_name = f\"models/stock_prediction_model_{version}_{addInfo}.keras\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## larger Model - not optimzed yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# 1st convolutional layer\n",
    "model.add(tf.keras.layers.Conv2D(16, (2, 2), activation='relu', padding='same'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "# 2nd convolutional layer\n",
    "model.add(tf.keras.layers.Conv2D(32, (2, 2), activation='relu', padding='same'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "# 3rd convolutional layer\n",
    "model.add(tf.keras.layers.Conv2D(64, (2, 2), activation='relu', padding='same'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "# FFNN\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(3, activation='sigmoid'))\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='auto')\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Wandele die Zielwerte in das One-Hot-Encoding-Format um\n",
    "y_train_encoded = to_categorical(y_train, num_classes=3)\n",
    "y_cv_encoded = to_categorical(y_cv, num_classes=3)\n",
    "y_test_encoded = to_categorical(y_test, num_classes=3)\n",
    "\n",
    "history = model.fit(x_train, y_train_encoded,\n",
    "                    validation_data=(x_cv, y_cv_encoded),\n",
    "                    epochs=200,\n",
    "                    callbacks=[early_stopping],\n",
    "                    class_weight={0: class_weights[0], 1: class_weights[1], class_weights[2]: 2})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_name = f\"models/stock_prediction_model_{version}_{addInfo}.keras\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histrory of the model\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title(f\"Model Accuracy - {version} - {addInfo}\")\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "# save the plot\n",
    "plot_name = f\"Plots/accuracy_plot_{version}_{addInfo}.png\"\n",
    "plt.savefig(plot_name)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: Checken!! Made with ChatGPT!!\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Vorhersagen für Validierungsdaten machen\n",
    "y_pred = model.predict(x_cv)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Confusion Matrix berechnen\n",
    "conf_matrix = confusion_matrix(y_cv, y_pred_classes)\n",
    "\n",
    "# Confusion Matrix plotten\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, \n",
    "            xticklabels=['Buy (0)', 'Sell (1)', 'Hold (2)'], \n",
    "            yticklabels=['Buy (0)', 'Sell (1)', 'Hold (2)'])\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title(f\"Confusion Matrix - {version} - {addInfo}\")\n",
    "# save the plot\n",
    "plot_name = f\"Plots/confusion_matrix_{version}_{addInfo}.png\"\n",
    "plt.savefig(plot_name)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_cv, y_pred_classes)\n",
    "print('Accuracy on validation data: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy on test data for Model 1\n",
    "y_test = np.array(y_test)\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print('Accuracy on test data: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorhersagen für Testset machen und Accuracy berechnen\n",
    "# Alternative Methode, da die Funktion evaluate() scheinbar ein Bug hat\n",
    "y_pred_test = model.predict(x_test)\n",
    "y_pred_test_classes = np.argmax(y_pred_test, axis=1)\n",
    "accuracy = accuracy_score(y_test, y_pred_test_classes)\n",
    "print('Accuracy on Test data: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy on test data for Model 2\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test_encoded)\n",
    "print('Accuracy on test data: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name                    Version                   Build  Channel\n",
    "absl-py                   2.1.0                    pypi_0    pypi\n",
    "altair                    5.3.0                    pypi_0    pypi\n",
    "appdirs                   1.4.4                    pypi_0    pypi\n",
    "appnope                   0.1.4              pyhd8ed1ab_0    conda-forge\n",
    "asttokens                 2.4.1              pyhd8ed1ab_0    conda-forge\n",
    "astunparse                1.6.3                    pypi_0    pypi\n",
    "attrs                     23.2.0                   pypi_0    pypi\n",
    "beautifulsoup4            4.12.3                   pypi_0    pypi\n",
    "blinker                   1.7.0                    pypi_0    pypi\n",
    "bzip2                     1.0.8                h93a5062_5    conda-forge\n",
    "ca-certificates           2024.2.2             hf0a4a13_0    conda-forge\n",
    "cachetools                5.3.3                    pypi_0    pypi\n",
    "certifi                   2024.2.2                 pypi_0    pypi\n",
    "cffi                      1.16.0                   pypi_0    pypi\n",
    "charset-normalizer        3.3.2                    pypi_0    pypi\n",
    "click                     8.1.7                    pypi_0    pypi\n",
    "clr-loader                0.2.6                    pypi_0    pypi\n",
    "coloredlogs               15.0.1                   pypi_0    pypi\n",
    "comm                      0.2.1              pyhd8ed1ab_0    conda-forge\n",
    "contourpy                 1.2.0                    pypi_0    pypi\n",
    "cycler                    0.12.1                   pypi_0    pypi\n",
    "dataclasses-json          0.5.14                   pypi_0    pypi\n",
    "debugpy                   1.8.1           py311h92babd0_0    conda-forge\n",
    "decorator                 5.1.1              pyhd8ed1ab_0    conda-forge\n",
    "exceptiongroup            1.2.0              pyhd8ed1ab_2    conda-forge\n",
    "executing                 2.0.1              pyhd8ed1ab_0    conda-forge\n",
    "filelock                  3.13.3                   pypi_0    pypi\n",
    "flatbuffers               24.3.25                  pypi_0    pypi\n",
    "fonttools                 4.49.0                   pypi_0    pypi\n",
    "frozendict                2.4.0                    pypi_0    pypi\n",
    "gast                      0.5.4                    pypi_0    pypi\n",
    "gitdb                     4.0.11                   pypi_0    pypi\n",
    "gitpython                 3.1.43                   pypi_0    pypi\n",
    "google-pasta              0.2.0                    pypi_0    pypi\n",
    "grpcio                    1.62.1                   pypi_0    pypi\n",
    "h5py                      3.11.0                   pypi_0    pypi\n",
    "html5lib                  1.1                      pypi_0    pypi\n",
    "humanfriendly             10.0                     pypi_0    pypi\n",
    "idna                      3.6                      pypi_0    pypi\n",
    "imbalanced-learn          0.12.2                   pypi_0    pypi\n",
    "imblearn                  0.0                      pypi_0    pypi\n",
    "importlib-metadata        7.0.1              pyha770c72_0    conda-forge\n",
    "importlib_metadata        7.0.1                hd8ed1ab_0    conda-forge\n",
    "ipykernel                 6.29.3             pyh3cd1d5f_0    conda-forge\n",
    "ipython                   8.22.1             pyh707e725_0    conda-forge\n",
    "jedi                      0.19.1             pyhd8ed1ab_0    conda-forge\n",
    "jinja2                    3.1.3                    pypi_0    pypi\n",
    "joblib                    1.3.2                    pypi_0    pypi\n",
    "jsonlines                 3.1.0                    pypi_0    pypi\n",
    "jsonschema                4.21.1                   pypi_0    pypi\n",
    "jsonschema-specifications 2023.12.1                pypi_0    pypi\n",
    "jupyter_client            8.6.0              pyhd8ed1ab_0    conda-forge\n",
    "jupyter_core              5.7.1           py311h267d04e_0    conda-forge\n",
    "keras                     3.2.1                    pypi_0    pypi\n",
    "kiwisolver                1.4.5                    pypi_0    pypi\n",
    "libclang                  18.1.1                   pypi_0    pypi\n",
    "libcxx                    16.0.6               h4653b0c_0    conda-forge\n",
    "libexpat                  2.5.0                hb7217d7_1    conda-forge\n",
    "libffi                    3.4.2                h3422bc3_5    conda-forge\n",
    "libsodium                 1.0.18               h27ca646_1    conda-forge\n",
    "libsqlite                 3.45.1               h091b4b1_0    conda-forge\n",
    "libzlib                   1.2.13               h53f4e23_5    conda-forge\n",
    "lxml                      5.1.0                    pypi_0    pypi\n",
    "markdown                  3.6                      pypi_0    pypi\n",
    "markdown-it-py            3.0.0                    pypi_0    pypi\n",
    "markupsafe                2.1.5                    pypi_0    pypi\n",
    "marshmallow               3.21.1                   pypi_0    pypi\n",
    "matplotlib                3.8.3                    pypi_0    pypi\n",
    "matplotlib-inline         0.1.6              pyhd8ed1ab_0    conda-forge\n",
    "mdurl                     0.1.2                    pypi_0    pypi\n",
    "ml-dtypes                 0.3.2                    pypi_0    pypi\n",
    "multitasking              0.0.11                   pypi_0    pypi\n",
    "mypy-extensions           1.0.0                    pypi_0    pypi\n",
    "namex                     0.0.7                    pypi_0    pypi\n",
    "ncurses                   6.4                  h463b476_2    conda-forge\n",
    "nest-asyncio              1.6.0              pyhd8ed1ab_0    conda-forge\n",
    "numpy                     1.26.4                   pypi_0    pypi\n",
    "openssl                   3.2.1                h0d3ecfb_0    conda-forge\n",
    "opt-einsum                3.3.0                    pypi_0    pypi\n",
    "optree                    0.11.0                   pypi_0    pypi\n",
    "packaging                 23.2               pyhd8ed1ab_0    conda-forge\n",
    "pandas                    2.1.4                    pypi_0    pypi\n",
    "pandas-ta                 0.3.14b0                 pypi_0    pypi\n",
    "parso                     0.8.3              pyhd8ed1ab_0    conda-forge\n",
    "peewee                    3.17.1                   pypi_0    pypi\n",
    "pexpect                   4.9.0              pyhd8ed1ab_0    conda-forge\n",
    "pickleshare               0.7.5                   py_1003    conda-forge\n",
    "pillow                    10.2.0                   pypi_0    pypi\n",
    "pip                       24.0               pyhd8ed1ab_0    conda-forge\n",
    "platformdirs              4.2.0              pyhd8ed1ab_0    conda-forge\n",
    "plotly                    5.20.0                   pypi_0    pypi\n",
    "prompt-toolkit            3.0.42             pyha770c72_0    conda-forge\n",
    "protobuf                  4.25.3                   pypi_0    pypi\n",
    "psutil                    5.9.8           py311h05b510d_0    conda-forge\n",
    "ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge\n",
    "pure_eval                 0.2.2              pyhd8ed1ab_0    conda-forge\n",
    "pyarrow                   14.0.2                   pypi_0    pypi\n",
    "pycparser                 2.21                     pypi_0    pypi\n",
    "pydeck                    0.8.1b0                  pypi_0    pypi\n",
    "pygments                  2.17.2             pyhd8ed1ab_0    conda-forge\n",
    "pyparsing                 3.1.1                    pypi_0    pypi\n",
    "python                    3.11.8          hdf0ec26_0_cpython    conda-forge\n",
    "python-dateutil           2.9.0.post0              pypi_0    pypi\n",
    "python_abi                3.11                    4_cp311    conda-forge\n",
    "pythonnet                 3.0.3                    pypi_0    pypi\n",
    "pytz                      2024.1                   pypi_0    pypi\n",
    "pyzmq                     25.1.2          py311h6727e71_0    conda-forge\n",
    "readline                  8.2                  h92ec313_1    conda-forge\n",
    "referencing               0.34.0                   pypi_0    pypi\n",
    "requests                  2.31.0                   pypi_0    pypi\n",
    "rich                      13.7.1                   pypi_0    pypi\n",
    "rpds-py                   0.18.0                   pypi_0    pypi\n",
    "scikit-learn              1.4.1.post1              pypi_0    pypi\n",
    "scipy                     1.12.0                   pypi_0    pypi\n",
    "seaborn                   0.13.2                   pypi_0    pypi\n",
    "setuptools                69.1.1             pyhd8ed1ab_0    conda-forge\n",
    "six                       1.16.0             pyh6c4a22f_0    conda-forge\n",
    "smmap                     5.0.1                    pypi_0    pypi\n",
    "soupsieve                 2.5                      pypi_0    pypi\n",
    "stack_data                0.6.2              pyhd8ed1ab_0    conda-forge\n",
    "streamlit                 1.33.0                   pypi_0    pypi\n",
    "ta                        0.11.0                   pypi_0    pypi\n",
    "tenacity                  8.2.3                    pypi_0    pypi\n",
    "tensorboard               2.16.2                   pypi_0    pypi\n",
    "tensorboard-data-server   0.7.2                    pypi_0    pypi\n",
    "tensorflow                2.16.1                   pypi_0    pypi\n",
    "tensorflow-io-gcs-filesystem 0.36.0                   pypi_0    pypi\n",
    "termcolor                 2.4.0                    pypi_0    pypi\n",
    "threadpoolctl             3.4.0                    pypi_0    pypi\n",
    "tk                        8.6.13               h5083fa2_1    conda-forge\n",
    "toml                      0.10.2                   pypi_0    pypi\n",
    "toolz                     0.12.1                   pypi_0    pypi\n",
    "tornado                   6.4             py311h05b510d_0    conda-forge\n",
    "tqdm                      4.66.2                   pypi_0    pypi\n",
    "tqdm-loggable             0.2                      pypi_0    pypi\n",
    "traitlets                 5.14.1             pyhd8ed1ab_0    conda-forge\n",
    "typing-inspect            0.9.0                    pypi_0    pypi\n",
    "typing_extensions         4.10.0             pyha770c72_0    conda-forge\n",
    "tzdata                    2024.1                   pypi_0    pypi\n",
    "urllib3                   2.2.1                    pypi_0    pypi\n",
    "wcwidth                   0.2.13             pyhd8ed1ab_0    conda-forge\n",
    "webencodings              0.5.1                    pypi_0    pypi\n",
    "werkzeug                  3.0.2                    pypi_0    pypi\n",
    "wheel                     0.42.0             pyhd8ed1ab_0    conda-forge\n",
    "wrapt                     1.16.0                   pypi_0    pypi\n",
    "xz                        5.2.6                h57fd34a_0    conda-forge\n",
    "yfinance                  0.2.37                   pypi_0    pypi\n",
    "zeromq                    4.3.5                hebf3989_1    conda-forge\n",
    "zipp                      3.17.0             pyhd8ed1ab_0    conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_name = f\"models/stock_prediction_model_v2.h5\"\n",
    "model = keras.models.load_model(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model as .keras\n",
    "model_name = f\"models/stock_prediction_model_v2.keras\"\n",
    "model.save(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download bitcoindata from yahoo finance\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# download the data\n",
    "data = yf.download('BTC-USD', start='2010-01-01', end='2021-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load selected features txt and save it in a list\n",
    "selected_features = np.loadtxt('models/selected_features_V2_mitSMOTE.txt', dtype=int).tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "banking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
